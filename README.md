<div align="center">

# ğŸ”¬ YOLO Vision Studio

**Real-time Object Detection Â· Segmentation Â· Pose Estimation Â· Tracking**
**Powered by YOLO26, YOLO World v2 & Streamlit**

[![Stars](https://img.shields.io/github/stars/CodingMantras/yolov8-streamlit-detection-tracking?style=for-the-badge&logo=github)](https://github.com/CodingMantras/yolov8-streamlit-detection-tracking/stargazers)
[![Python](https://img.shields.io/badge/Python-3.9+-blue?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.40+-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)](https://streamlit.io)
[![Ultralytics](https://img.shields.io/badge/Ultralytics-8.3+-purple?style=for-the-badge)](https://ultralytics.com)
[![License](https://img.shields.io/github/license/aparsoft/yolov8-streamlit-detection-tracking?style=for-the-badge)](LICENSE)

[Live Demo](https://yolov8-object-detection-and-tracking-app.streamlit.app/) Â· [Blog Series](https://rs-punia.medium.com/building-a-real-time-object-detection-and-tracking-app-with-yolov8-and-streamlit-part-1-30c56f5eb956) Â· [Report Bug](https://github.com/CodingMantras/yolov8-streamlit-detection-tracking/issues)

</div>

---

## ğŸ†• What's New in v2.0

> **Thank you for 400+ â­ stars!** This major update brings a completely rewritten, modular codebase with exciting new capabilities.

| Feature | v1.0 | v2.0 |
|---------|------|------|
| Object Detection | YOLOv8n | YOLO26n (NMS-free, 43% faster CPU) |
| Segmentation | YOLOv8n-seg | YOLO26n-seg (multi-scale proto + semantic loss) |
| Pose Estimation | âŒ | âœ… YOLO26n-pose (RLE-based keypoints) |
| Open-Vocabulary | âŒ | âœ… YOLO World v2 (natural language text prompts) |
| Tracking | Basic | ByteTrack + BoTSORT with local + global counting |
| Object Counting | âŒ | âœ… Per-frame local + cumulative global counts |
| Skip Frames | âŒ | âœ… 1â€“8Ã— skip for fast inference on long videos |
| Webcam | OpenCV (broken in cloud) | âœ… streamlit-webrtc (browser-native) |
| Architecture | Monolithic | Modular service-based design |
| Video Metrics | âŒ | âœ… Live FPS, local/global counts & tracking overlay |
| Codebase | `helper.py + settings.py` | `config Â· model_loader Â· image_service Â· video_service` |

---

## âœ¨ Features

### ğŸ“· Image Inference
- **Object Detection** â€” Detect 80+ COCO classes with YOLO26 (NMS-free, edge-optimized)
- **YOLO World v2 (Text Prompt)** â€” Natural language prompts like *"person in black"*, *"red car"*, *"laptop on table"* for open-vocabulary detection
- **Instance Segmentation** â€” Pixel-level object segmentation with multi-scale proto modules
- **Pose Estimation** â€” Human body keypoint and skeleton detection with RLE precision
- Per-class metrics, confidence scores and detailed results table

### ğŸ¬ Video Inference
- **Multiple Sources**: Stored videos, Webcam (browser-native via WebRTC), RTSP streams, YouTube URLs
- **Real-time Tracking**: ByteTrack and BoTSORT algorithms (enabled by default)
- **Local + Global Counting**: Per-frame counts (green) and cumulative unique-object counts (yellow) displayed on every frame
- **Skip Frames**: Adjustable 1â€“8Ã— slider for faster inference on long or high-FPS videos
- **YOLO World v2 in Video**: Natural language text-prompt search in video streams
- **Live Metrics**: Separate local (this frame) and global (cumulative) sections in sidebar
- **Count Overlay**: Two-line on-frame badge â€” local in green, global in yellow

### ğŸ—ï¸ Architecture
- **Modular Design**: Separate services for image and video inference
- **Centralized Config**: Single `config.py` for all settings
- **Cached Models**: `@st.cache_resource` for instant model reuse
- **Clean Routing**: Task + Mode based dispatch in `app.py`

---

## ğŸ“¸ Demo

### Tracking with Object Detection
<https://user-images.githubusercontent.com/104087274/234874398-75248e8c-6965-4c91-9176-622509f0ad86.mov>

### Application Overview
<https://github.com/user-attachments/assets/85df351a-371c-47e0-91a0-a816cf468d19.mov>

### Screenshots

| Home Page | Detection Result | Segmentation |
|:---------:|:----------------:|:------------:|
| <img src="https://github.com/CodingMantras/yolov8-streamlit-detection-tracking/blob/master/assets/pic1.png" width="300"> | <img src="https://github.com/CodingMantras/yolov8-streamlit-detection-tracking/blob/master/assets/pic3.png" width="300"> | <img src="https://github.com/CodingMantras/yolov8-streamlit-detection-tracking/blob/master/assets/segmentation.png" width="300"> |

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9 or higher
- GPU recommended (NVIDIA CUDA) for real-time video inference
- Webcam (optional, for live detection)

### Installation

```bash
# 1. Clone the repository
git clone https://github.com/CodingMantras/yolov8-streamlit-detection-tracking.git
cd yolov8-streamlit-detection-tracking

# 2. Create a virtual environment
python -m venv venv
source venv/bin/activate        # Linux / macOS
# venv\Scripts\activate         # Windows

# 3. Install dependencies
pip install -r requirements.txt
```

### Download Model Weights

The default detection and segmentation weights are auto-downloaded by Ultralytics on first use. All YOLO26 models (detection, segmentation, pose) and YOLO World v2 (open-vocabulary) are fetched automatically.

To pre-download manually:

```bash
# Detection
wget -P weights/ https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt

# Segmentation
wget -P weights/ https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-seg.pt

# Pose estimation
wget -P weights/ https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n-pose.pt

# YOLO World v2 open-vocabulary (auto-downloads if not present)
wget -P weights/ https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l-worldv2.pt
```

### Run the App

```bash
streamlit run app.py
```

The app opens at **http://localhost:8501**.

---

## ğŸ“– Usage Guide

### Sidebar Controls

1. **Inference Mode** â€” Choose between ğŸ“· *Image Inference* or ğŸ¬ *Video Inference*
2. **Task** â€” Select one of:
   - **Object Detection** â€” Standard YOLO26 object detection (NMS-free, end-to-end)
   - **Segmentation** â€” Instance segmentation with pixel masks
   - **YOLO World v2 (Text Prompt)** â€” Open-vocabulary detection with natural language prompts
   - **Pose Estimation** â€” Human body keypoint detection
3. **Model Confidence** â€” Adjust the confidence threshold (10â€“100%)

### Image Inference

1. Select **ğŸ“· Image Inference** mode
2. Choose a task (Detection, Segmentation, YOLO World, or Pose)
3. Upload an image or use the default
4. For **YOLO World v2**: type descriptive phrases (e.g., `person in black, red car, laptop on table`)
5. Click **ğŸš€ Run** to see results with per-class metrics

### Video Inference

1. Select **ğŸ¬ Video Inference** mode
2. Choose a task
3. Pick a video source: **Stored Video**, **Webcam**, **RTSP**, or **YouTube**
4. **Object Tracking** is enabled by default (ByteTrack or BoTSORT) â€” local + global counts display automatically
5. Adjust **Skip Frames** (1â€“8) in the sidebar for faster inference on long videos
6. For **YOLO World v2**: enter natural language prompts to search for in the video
7. Click **ğŸš€ Detect** â€” local and global metrics appear in the sidebar

### Adding Your Own Videos

Drop `.mp4` files into the `videos/` directory. They appear automatically in the stored-video dropdown â€” no code changes required (the config scans the folder at startup).

---

## ğŸ—‚ï¸ Project Structure

```
yolov8-streamlit-detection-tracking/
â”œâ”€â”€ app.py                # Main Streamlit application & routing
â”œâ”€â”€ config.py             # Centralized configuration (paths, models, UI)
â”œâ”€â”€ model_loader.py       # Model loading with @st.cache_resource
â”œâ”€â”€ image_service.py      # Image inference (detection, segmentation, world, pose)
â”œâ”€â”€ video_service.py      # Video inference (tracking, counting, all sources)
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ packages.txt          # System packages for Streamlit Cloud
â”œâ”€â”€ README.md
â”œâ”€â”€ assets/               # Screenshots and demo media
â”œâ”€â”€ images/               # Sample images
â”œâ”€â”€ videos/               # Sample videos (add your .mp4 files here)
â””â”€â”€ weights/              # Model weights (yolov8n.pt, yolov8n-seg.pt, ...)
```

### Module Responsibilities

| Module | Purpose |
|--------|---------|
| `config.py` | All paths, model names, UI constants, and default values |
| `model_loader.py` | Cached model loading; resolves local weights vs auto-download |
| `image_service.py` | Full image-mode UI: upload â†’ inference â†’ results display |
| `video_service.py` | Full video-mode UI: source selection â†’ frame loop â†’ live metrics |
| `app.py` | Page config, sidebar, and routing to the correct service |

---

## âš™ï¸ Configuration

All configuration lives in `config.py`. Key settings:

```python
# Models â€” change to larger variants for better accuracy
DETECTION_MODEL    = "yolo26n.pt"        # or yolo26s.pt, yolo26m.pt, yolo26l.pt
SEGMENTATION_MODEL = "yolo26n-seg.pt"    # or yolo26s-seg.pt
YOLO_WORLD_MODEL   = "yolov8l-worldv2.pt" # open-vocabulary (natural language)
POSE_MODEL         = "yolo26n-pose.pt"   # or yolo26s-pose.pt

# Inference defaults
DEFAULT_CONFIDENCE = 0.40
DEFAULT_IOU        = 0.50
VIDEO_DISPLAY_WIDTH = 720

# Skip-frame control for video inference
DEFAULT_SKIP_FRAMES = 1   # process every frame (1â€“8)

# YOLO World v2 default prompts
DEFAULT_WORLD_CLASSES = "person in black, red car, dog, laptop on table"
```

### Custom Models

To use your own trained model:

```python
# In config.py
DETECTION_MODEL = "my_custom_model.pt"
# Place the .pt file in the weights/ directory
```

---

## â˜ï¸ Deploy to Streamlit Cloud

1. Push the repository to GitHub
2. Go to [share.streamlit.io](https://share.streamlit.io) and connect your repo
3. Set the main file path to `app.py`
4. The `packages.txt` file handles system-level dependencies automatically

> **Note**: Streamlit Cloud has no GPU â€” video inference will be slower. Image inference works well. Webcam uses **streamlit-webrtc** so it works natively in the browser (no server-side camera access needed).

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how:

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m "Add amazing feature"`
4. Push: `git push origin feature/amazing-feature`
5. Open a Pull Request

### Ideas for Contributions

- [ ] Add model benchmarking / comparison page
- [ ] Export detection results to CSV / JSON
- [ ] Add YOLO-NAS or RT-DETR model support
- [ ] Region of Interest (ROI) based counting
- [ ] Multi-camera RTSP dashboard

---

## ğŸ“š Resources

- [Ultralytics YOLO26 Documentation](https://docs.ultralytics.com/models/yolo26/)
- [YOLO World v2 Documentation](https://docs.ultralytics.com/models/yolo-world/)
- [streamlit-webrtc](https://github.com/whitphx/streamlit-webrtc) â€” Browser-native webcam
- [Streamlit Documentation](https://docs.streamlit.io/)
- [ByteTrack Paper](https://arxiv.org/abs/2110.06864)
- [Blog Series â€” Building this App](https://medium.com/@mycodingmantras/building-a-real-time-object-detection-and-tracking-app-with-yolov8-and-streamlit-part-1-30c56f5eb956)

---

## ğŸ“„ License

This project is open-source and available for educational and research purposes.

## ğŸ™ Acknowledgements

- [Ultralytics](https://github.com/ultralytics/ultralytics) for YOLO26 and YOLO World v2
- [Streamlit](https://github.com/streamlit/streamlit) for the web framework
- [streamlit-webrtc](https://github.com/whitphx/streamlit-webrtc) for browser-based webcam
- All **400+** stargazers for the love and support!

---

<div align="center">

**If you find this project useful, please consider giving it a â­!**

Made with â¤ï¸ by [Aparsoft](https://aparsoft.com/)

</div>
